{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b16766-f022-46f1-8e12-185c7c754597",
   "metadata": {},
   "source": [
    "# LABORATORY 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a5bab-8b50-4b82-bc4b-313fecbc0789",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Run a simple Spark job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be3995b-dbd6-4bda-bb2e-c7bec23a0e3d",
   "metadata": {},
   "source": [
    "### 1.1 Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a625b3ec-6e71-4eb5-bfa5-715f86e629f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum is: 46\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\")\n",
    "fields_rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "value_rdd = fields_rdd.map(lambda l: int(l[1]))\n",
    "value_sum = value_rdd.reduce(lambda v1, v2: v1+v2)\n",
    "print(\"The sum is:\", value_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dedfd4-a6c2-4f25-a87e-bfbfd61d92e8",
   "metadata": {},
   "source": [
    "#### 1.1.1 Which value is printed by the print statement?\n",
    "The printed value is 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f24d12-bbbf-4538-8719-5ab84dc9382c",
   "metadata": {},
   "source": [
    "#### 1.1.2 Which is the purpose of each line of code?\n",
    "The first line creates an RDD, using the textFile method, from the text file lab1_dataset.txt. In the second line the map method is used to  split the two values separated by the comma, and create a list of list where the first element is a name and second element is a number.\n",
    "The value_rdd contains a list of numbers properly cast into integers using the map method.\n",
    "Finally a sum of all the values is done by the reduce method and the result is printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5b863b-814e-4f82-86e5-601bb0f78f2d",
   "metadata": {},
   "source": [
    "#### 1.1.3 Can you see your job at Hue? \n",
    "At HUE no jobs are seen beacuse we are running pyspark locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631f31c-b473-4d30-b448-61d6b0b8e3a1",
   "metadata": {},
   "source": [
    "#### 1.1.4 What changes between Pyspark (local) and Pyspark (Yarn)? Who is taking more time?\n",
    "Pyspark local runs on the server dedicated to jupyter.polito.it, while PySpark YARN runs in the cluster. PySpark YARN takes more time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13f6b2-2692-4e47-8fa2-2884879d55f8",
   "metadata": {},
   "source": [
    "#### 1.1.5 Can you see your job at Hue? What changed? \n",
    "Now the job is present in HUE beacause the executors are working in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce7ca8b-0b34-4841-971e-0bde1a858ad2",
   "metadata": {},
   "source": [
    "### 1.2 Execute in a pyspark shell "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd86a4ee-e105-445c-808d-35ebbb4bc803",
   "metadata": {},
   "source": [
    "#### 1.2.1 What does `--master local` mean? What is the equivalent Pyspark kernel?\n",
    "*--master local* specify that the scheduler used to run the application is executed locally on the server of jupyter.polito.it. In this case the PySpark kernel is local."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f4d514-7234-4799-98ca-a91edf9ceec1",
   "metadata": {},
   "source": [
    "#### 1.2.2 What does `--deploy-mode client` mean? Where do spark executors and driver are executed?\n",
    "*--deploy-mode client* means that the driver is executed in the gateway. The executors are executed on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32571a95-cbed-4e7d-9443-53b5c4fbf09b",
   "metadata": {},
   "source": [
    "### 1.3 Create a Spark script and run it from the command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4489402-3680-4447-96c6-759ea28bf13c",
   "metadata": {},
   "source": [
    "#### 1.3.1 In which file systems are located your script and the `/data/students/bigdata_internet/lab1/lab1_dataset.txt` files?\n",
    "The script is located on the jupyter.polito server filesystem, while */data/students/bigdata_internet/lab1/lab1_dataset.txt* file is on the cluster in the hdfs file system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6b4dc-f4f4-4ba3-b1d5-f6b0fbfb53f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Play with HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ea246-b261-4c13-8b9b-fcc97bb4ffea",
   "metadata": {},
   "source": [
    "### 2.1 If you modify the local file, does the modifications automatically affect also the HDFS file?\n",
    "No, the changes are saved on the local machine and don't affect the hdfs file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9906f-1e44-471f-af52-64ee93dc6731",
   "metadata": {},
   "source": [
    "### 2.2 Which is the complete path of your file in HDFS? And on the gateway local file system?\n",
    "The complete path on HDFS is */user/s309709/empty.txt*, while on local filesystem  is */home/students/s309709/LAB01/empty.txt*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11139b71-c20f-423d-beee-a375a1688be4",
   "metadata": {},
   "source": [
    "## 3. Run a Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9544d6de-ea07-4683-b519-1b969eec8a41",
   "metadata": {},
   "source": [
    "### 3.1 Report the code you have written, and explain the goal of each instruction\n",
    "At first an RDD is created using the method textFile.\n",
    "In the second line the map method is used to split the two values separated by the comma, and to create a list of list where the first element is a name and second element is a number.\n",
    "Then using reduceByKey a sum of of the numebers related to the same key, a name in this case, is evaluated.\n",
    "Finally by the map method the key and number are put into a string to be printed and the saved in the HDSF using saveAsTextFile method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38067329-8c96-49b6-a322-72df6236fc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bob, 11', 'john, 21', 'alice, 14']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\")\n",
    "fields_rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "sumByname = fields_rdd.reduceByKey(lambda v1,v2: int(v1)+int(v2))\n",
    "toprint_rdd = sumByname.map(lambda l: f\"{l[0]}, {l[1]}\")\n",
    "toprint_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a1bb1f2-8e06-4b78-a9dd-bb3ad8cf651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "toprint_rdd.saveAsTextFile(\"/user/s309709/LAB01/es3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35850ba-596d-4d04-b883-d4798ae92917",
   "metadata": {},
   "source": [
    "### 3.2 How does the output folder on `HDFS` look like? Why do you find multiple `parts` file in the folder?\n",
    "Into the output folder (es3 in this case) there are files: SUCCESS, part-00000, part-00001.\n",
    "The first file purpose is just to confirm that the saving was successful and no errors occured.\n",
    "While part-00000 and part-00001 are the two partition in which the RDD saved the data; in my case Bob and John are saved on the first partition, while alice on the second one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd528e-3a56-4960-a72a-bf0d3f4c5b48",
   "metadata": {},
   "source": [
    "## 4. Bonus Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb776326-e18e-4a8a-91ae-a53ed2f99bbb",
   "metadata": {},
   "source": [
    "### 4.1 Report the code you have written, and explain the goal of each instruction.\n",
    "The first line create an RDD from a text file. Then using the map method each line is split by the comma.\n",
    "After that ReduceByKey method is applied to separate the values associated to each key (the age in this case) with *'-'*.\n",
    "Finally using the map method the keys and the values have been put together in the requested formatting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9237c804-758e-4805-b72d-818b3428696e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bob,5-3-3', 'john,4-8-9', 'alice,4-3-7']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\")\n",
    "fields_rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "concatenate_rdd = fields_rdd.reduceByKey(lambda v1, v2: f\"{v1}-{v2}\")\n",
    "toprint_rdd = concatenate_rdd.map(lambda l: f\"{l[0]},{l[1]}\")\n",
    "toprint_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faa90ebd-2c2c-4856-9470-013f963478e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "toprint_rdd.saveAsTextFile(\"/user/s309709/LAB01/es4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
